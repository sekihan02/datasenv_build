{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Generate Music using a LSTM Neural Network in PyTorch\n",
    "\n",
    "[How to Generate Music using a LSTM Neural Network in Keras](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)を訳しながらPyTorchで実装を目指します。\n",
    "\n",
    "データセットは[The Largest MIDI Collection on the Internet](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/)\n",
    "- 公開されているMIDIデータを収集した大規模なデータセット(※もちろん有料なコンテンツは含まれない)。\n",
    "- ポップ、クラシック、ゲーム音楽など多彩なジャンルで構成されており、総ファイル数13万・約100時間分のデータとなっている。\n",
    "- Tronto大学の[Song From PI](http://www.cs.toronto.edu/songfrompi/)で使用されたデータセット\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "近年、ニューラルネットワークを使ってテキストを生成する方法についてのチュートリアルは数多くありますが、音楽を作成する方法についてのチュートリアルは不足しています。\n",
    "この記事では、Kerasライブラリを使ってPythonでリカレントニューラルネットワークを使って音楽を作成する方法を解説します。\n",
    "せっかちな方のために、チュートリアルの最後にGithubリポジトリへのリンクがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "実装の詳細に入る前に、明確にしておかなければならない用語があります。\n",
    "\n",
    "### LSTM\n",
    "このチュートリアルでは、Long Short-Term Memory (LSTM)ネットワークを使用します。\n",
    "LSTMはリカレント・ニューラル・ネットワークの一種で、勾配降下を介して効率的に学習することができます。\n",
    "ゲーティング・メカニズムを用いて、LSTMは長期パターンを認識し、符号化することができます。\n",
    "LSTMは、音楽やテキスト生成のように、ネットワークが長期間にわたって情報を記憶しなければならないような問題を解決するのに非常に有用です。\n",
    "\n",
    "### Music21\n",
    "Music21は、コンピュータ支援音楽学に使われるPythonのツールキットです。音楽理論の基礎を教えたり、譜例を生成したり、音楽を勉強したりすることができます。\n",
    "このツールキットは、MIDIファイルの楽譜を取得するためのシンプルなインターフェースを提供します。\n",
    "さらに、ノートとコードオブジェクトを作成して、自分のMIDIファイルを簡単に作成することができます。\n",
    "このチュートリアルでは、Music21を使ってデータセットの内容を抽出し、ニューラルネットワークの出力を取り、楽譜に変換します。\n",
    "\n",
    "### トレーニング\n",
    "このセクションでは、モデルのデータをどのように収集したか、LSTMモデルで使用できるようにデータをどのように準備したか、モデルのアーキテクチャについて説明します。\n",
    "データ\n",
    "私たちのGithubリポジトリでは、主にファイナルファンタジーのサウンドトラックからの音楽で構成されたピアノ音楽を使用しました。私たちがファイナルファンタジーの音楽を選んだのは、作品の大部分が持っている非常にはっきりとした美しいメロディーと、存在する作品の量の多さのためです。しかし、単一の楽器で構成されたMIDIファイルのセットであれば、どのようなものでも構いません。\n",
    "ニューラルネットワークを実装するための最初のステップは、作業するデータを調べることです。\n",
    "\n",
    "下の図は、Music21を使って読み込んだMIDIファイルの抜粋です。\n",
    "\n",
    "```\n",
    "<music21.note.Note B> 72.0\n",
    "<music21.chord.Chord E3 A3> 72.0\n",
    "<music21.note.Note A> 72.5\n",
    "<music21.chord.Chord E3 A3> 72.5\n",
    "<music21.note.Note E> 73.0\n",
    "<music21.chord.Chord E3 A3> 73.0\n",
    "<music21.chord.Chord E3 A3> 73.5\n",
    "<music21.note.Note E-> 74.0\n",
    "<music21.chord.Chord F3 A3> 74.0\n",
    "<music21.chord.Chord F3 A3> 74.5\n",
    "<music21.chord.Chord F3 A3> 75.0\n",
    "<music21.chord.Chord F3 A3> 75.5\n",
    "<music21.chord.Chord E3 A3> 76.0\n",
    "<music21.chord.Chord E3 A3> 76.5\n",
    "<music21.chord.Chord E3 A3> 77.0\n",
    "<music21.chord.Chord E3 A3> 77.5\n",
    "<music21.chord.Chord F3 A3> 78.0\n",
    "<music21.chord.Chord F3 A3> 78.5\n",
    "<music21.chord.Chord F3 A3> 79.0\n",
    "```\n",
    "\n",
    "この抜粋とデータセットのほとんどからわかるように、ミディファイルのノート間の最も一般的な間隔は 0.5 です。したがって、可能な出力のリストの中の様々なオフセットを無視することで、データとモデルを単純化することができます。\n",
    "これは、ネットワークによって生成された音楽のメロディーにあまり深刻な影響を与えません。\n",
    "そこで、このチュートリアルではオフセットを無視して、可能な出力のリストを352にしておきます。\n",
    "\n",
    "#### データの準備\n",
    "データを調べて、LSTMネットワークの入力と出力として使用したい機能が音符と和音であることがわかったので、ネットワーク用のデータを準備します。\n",
    "\n",
    "まず、以下のコードスニペットにあるように、データを配列に読み込みます。\n",
    "\n",
    "```\n",
    "from music21 import converter, instrument, note, chord\n",
    "notes = []\n",
    "for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "    midi = converter.parse(file)\n",
    "    notes_to_parse = None\n",
    "    parts = instrument.partitionByInstrument(midi)\n",
    "    if parts: # file has instrument parts\n",
    "        notes_to_parse = parts.parts[0].recurse()\n",
    "    else: # file has notes in a flat structure\n",
    "        notes_to_parse = midi.flat.notes\n",
    "    for element in notes_to_parse:\n",
    "        if isinstance(element, note.Note):\n",
    "            notes.append(str(element.pitch))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "```\n",
    "\n",
    "まず、converter.parse(file)関数を使って、各ファイルをMusic21のストリームオブジェクトにロードします。\n",
    "このストリームオブジェクトを使って、ファイル内のすべての音符と和音のリストを取得します。\n",
    "音の最も重要な部分は音程の文字列表記を使って再現できるので、すべての音符オブジェクトの音程を文字列表記を使って追加します。\n",
    "そして、コードの中のすべてのノートのIDを、それぞれのノートをドットで区切って1つの文字列にエンコードすることで、すべてのコードを追加します。\n",
    "これらのエンコーディングにより、ネットワークで生成された出力を正しい音と和音に簡単にデコードすることができます。\n",
    "\n",
    "これで、すべての音符と和音をシーケンシャルリストにまとめたので、ネットワークの入力となるシーケン スを作成できます。\n",
    "\n",
    "![midimusic.jpg](./images/midimusic.jpg)\n",
    "\n",
    "<center>図1: カテゴリから数値データに変換するとき、データは、カテゴリが明確な値のセットの中でどこに位置しているかを表す整数のインデックスに変換されます。例えば、リンゴは最初の明確な値なので0にマップされ、オレンジは2番目なので1にマップされ、パイナップルは3番目なので2にマップされます。</center>\n",
    "\n",
    "まず、文字列ベースのカテゴリデータから整数ベースの数値データへのマッピング関数を作成します。\n",
    "これは、ニューラルネットワークが文字列ベースのカテゴリデータよりも整数ベースの数値データの方が性能が良いためです。\n",
    "カテゴリデータから数値データへの変換の例を図1に示します。\n",
    "\n",
    "\n",
    "次に、ネットワークの入力シーケンスとそれぞれの出力を作成します。\n",
    "各入力シーケンスの出力は、入力シーケンスの音符のリストの中で、入力シーケンスの音符のシーケンスの後に来る最初の音符または和音になります。\n",
    "\n",
    "```python\n",
    "sequence_length = 100\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "network_input = []\n",
    "network_output = []\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "    sequence_in = notes[i:i + sequence_length]\n",
    "    sequence_out = notes[i + sequence_length]\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    network_output.append(note_to_int[sequence_out])\n",
    "n_patterns = len(network_input)\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "# normalize input\n",
    "network_input = network_input / float(n_vocab)\n",
    "network_output = np_utils.to_categorical(network_output)\n",
    "```\n",
    "\n",
    "このコード例では、各シーケンスの長さを100音/和音としています。\n",
    "これは、シーケンスの次の音を予測するために、ネットワークが予測を行うのに役立つ前の100音を持っていることを意味します。\n",
    "シーケンスの長さの違いがネットワークによって生成された音楽に与える影響を確認するために、異なるシーケンスの長さを使用してネットワークをトレーニングすることを強くお勧めします。\n",
    "ネットワーク用のデータを準備する最後のステップは、入力を正規化し、出力を[ワンホットエンコード](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)することです。\n",
    "\n",
    "## Model\n",
    "\n",
    "最後に、モデルアーキテクチャの設計に入ります。\n",
    "このモデルでは、4つの異なるタイプのレイヤーを使用しています。\n",
    "\n",
    "LSTM層は、シーケンスを入力として受け取り、シーケンス(return_sequences=True)または行列を返すリカレント・ニューラル・ネット層です。\n",
    "\n",
    "ドロップアウト層は正則化技術で、オーバーフィットを防ぐために、訓練中の各更新時に入力単位の端数を0に設定することで構成されています。\n",
    "この割合は，その層で使用されるパラメータによって決定されます。\n",
    "\n",
    "密層(Dense layers)または完全接続層(fully connected layers)は、各入力ノードが各出力ノードに接続されている完全接続型のニューラルネットワーク層です。\n",
    "\n",
    "活性化層は、我々のニューラルネットワークがノードの出力を計算するために使用する活性化関数を決定します。\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        256,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "```\n",
    "これで、使用するさまざまなレイヤについての情報が得られたので、それらをネットワークモデルに追加します。\n",
    "\n",
    "各LSTM, Dense, Activation層の最初のパラメータは、その層が持つべきノードの数です。ドロップアウト層では、最初のパラメータは学習中にドロップアウトされる入力ユニットの割合です。\n",
    "\n",
    "最初のレイヤーでは、input_shapeと呼ばれるユニークなパラメータを与えなければなりません。このパラメータの目的は、学習するデータの形状をネットワークに知らせることです。\n",
    "\n",
    "最後の層には、システムが持つ異なる出力の数と同じ数のノードを常に含まなければなりません。これにより、ネットワークの出力がクラスに直接対応することが保証されます。\n",
    "\n",
    "このチュートリアルでは、3つのLSTM層、3つのドロップアウト層、2つの密層、1つの活性化層からなる単純なネットワークを使用します。予測の質を向上させることができるかどうかを確認するために、ネットワークの構造で遊んでみることをお勧めします。\n",
    "\n",
    "学習の各反復の損失を計算するために、我々の出力のそれぞれが単一のクラスに属するだけであり、作業するクラスが2つ以上あるので、カテゴリークロスエントロピーを使用します。そして、ネットワークを最適化するためにRMSpropオプティマイザを使用します。\n",
    "\n",
    "```python\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,        \n",
    "    mode='min'\n",
    ")    \n",
    "callbacks_list = [checkpoint]     \n",
    "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n",
    "```\n",
    "ネットワークのアーキテクチャを決定したら、トレーニングを開始します。\n",
    "Kerasのmodel.fit()関数を使ってネットワークを学習します。最初のパラメータは先ほど準備した入力シーケンスのリストで、2番目のパラメータはそれぞれの出力のリストです。\n",
    "このチュートリアルでは、ネットワークを200エポック（反復）、ネットワークを伝搬する各バッチには64個のサンプルが含まれています。\n",
    "\n",
    "努力の成果を失うことなく、いつでも学習を中止できるようにするために、モデル・チェックポイントを使用します。モデル・チェックポイントは、エポックごとにネットワーク・ノードの重みをファイルに保存する方法を提供してくれます。\n",
    "これにより、重みを失うことを心配することなく、損失値に満足したらニューラル・ネットワークの実行を停止することができます。\n",
    "そうでなければ、重みをファイルに保存する機会を得る前に、ネットワークが200エポックすべてを通過し終わるまで待たなければなりません。\n",
    "\n",
    "## 音楽の生成\n",
    "ネットワークのトレーニングが終わったので、今まで何時間もかけてトレーニングしてきたネットワークを楽しんでみましょう。\n",
    "ニューラルネットワークを使って音楽を生成できるようにするには、以前と同じ状態にする必要があります。\n",
    "簡単にするために、トレーニングセクションのコードを再利用してデータを準備し、前と同じ方法でネットワークモデルをセットアップします。ただし、ネットワークをトレーニングする代わりに、トレーニングセクションで保存した重みをモデルにロードします。\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    512,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "# Load the weights to each node\n",
    "model.load_weights('weights.hdf5')\n",
    "```\n",
    "ここで、訓練されたモデルを使用して、ノートの生成を開始できます。\n",
    "\n",
    "自由に使えるノートシーケンスの完全なリストがあるので、リスト内のランダムなインデックスを開始点として選択します。\n",
    "しかし、開始点を制御したい場合は、ランダム関数をコマンドライン引数に置き換えてください。\n",
    "\n",
    "ここでは、ネットワークの出力をデコードするためのマッピング関数も作成する必要があります。\n",
    "この関数は数値データからカテゴリデータ（整数からノートまで）へのマッピングを行います。\n",
    "\n",
    "```python\n",
    "start = numpy.random.randint(0, len(network_input)-1)\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "# generate 500 notes\n",
    "for note_index in range(500):\n",
    "    prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction_input = prediction_input / float(n_vocab)\n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "````\n",
    "私たちは、ネットワークを使って500音を生成することにしました。\n",
    "生成したい音符ごとに、ネットワークにシーケンスを送信しなければなりません。\n",
    "最初に提出するシーケンスは、開始インデックスにある音符のシーケンスです。\n",
    "それ以降のシーケンスを入力として使用するたびに、図2に示すように、シーケンスの最初の音符を削除し、シーケンスの最後に前の反復の出力を挿入します。\n",
    "\n",
    "![sequence](./images/sequence.jpg)\n",
    "<center>図2: 最初の入力シーケンスはABCDEです。次の反復では、シーケンスからAを削除してFを追加します。そして、この処理を繰り返します。</center>\n",
    "\n",
    "ネットワークからの出力から最も可能性の高い予測を決定するために、最も高い値のインデックスを抽出します。\n",
    "出力配列のインデックスXの値は、Xが次の音符である確率に対応しています。\n",
    "これを説明するのに役立つのが図3です。\n",
    "\n",
    "![mapping](./images/mapping.jpg)\n",
    "<center>図3: ここでは、ネットワークからの出力予測とクラスの間のマッピングを示しています。最も高い確率で次の値がDであることがわかるので、最も確率の高いクラスとしてDを選択します。</center>\n",
    "\n",
    "次に、ネットワークからのすべての出力を 1 つの配列に集めます。\n",
    "\n",
    "これで、音符と和音のエンコードされた表現がすべて配列になったので、それらをデコードして、音符と和音のオブジェクトの配列を作成することができます。\n",
    "\n",
    "まず、デコードしている出力がノートなのかコードなのかを判断しなければなりません。\n",
    "\n",
    "パターンがコードの場合、文字列を音符の配列に分割しなければなりません。そして、それぞれのノートの文字列表現をループして、それぞれのノートのノートオブジェクトを作成します。そして、これらのノートのそれぞれを含むコードオブジェクトを作成します。\n",
    "\n",
    "パターンがノートの場合は、パターンに含まれる音程の文字列表現を使用してノートオブジェクトを作成します。\n",
    "\n",
    "各反復の最後に、オフセットを0.5増加させ（前節で決めたように）、作成したNote/Chordオブジェクトをリストに追加します。\n",
    "\n",
    "```python\n",
    "offset = 0\n",
    "output_notes = []\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "```\n",
    "ネットワークで生成された音符と和音のリストができたので、このリストをパラメータにして Music21 Stream オブジェクトを作成します。\n",
    "最後に、ネットワークで生成された音楽を含むMIDIファイルを作成するために、Music21ツールキットのwrite関数を使って、ストリームをファイルに書き出します。\n",
    "\n",
    "```python\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "midi_stream.write('midi', fp='test_output.mid')\n",
    "```\n",
    "## 結果\n",
    "さて、その結果に驚嘆する時が来ました。\n",
    "図4は、LSTMネットワークを使用して生成された音楽の楽譜表現を含んでいます。\n",
    "ぱっと見ただけで、それにはいくつかの構造があることがわかります。\n",
    "これは、2ページ目の3行目から最後の行で特に明らかです。\n",
    "\n",
    "音楽の知識があり、楽譜を読むことができる人は、楽譜に奇妙な音符が散らばっていることがわかります。\n",
    "これはニューラルネットワークが完璧なメロディーを作ることができない結果です。\n",
    "現在の実装では、常にいくつかの偽の音符があり、より良い結果を得るためには、より大きなネットワークが必要になります。\n",
    "\n",
    "![example_of_sheet_music](./images/example_of_sheet_music.png)\n",
    "<center>図4：LSTMネットワークで生成された楽譜の一例</center>\n",
    "\n",
    "この比較的浅いネットワークの結果は、Embed 1の音楽の例を見ても分かるように、非常に印象的です。 \n",
    "興味のある方は、図4の楽譜は、NeuralNet Music 5の楽譜を表しています。\n",
    "\n",
    "## 今後の仕事\n",
    "簡単なLSTMネットワークと352クラスを使って、目覚ましい成果と美しいメロディを実現しています。\n",
    "しかし、改善できる部分もあります。\n",
    "\n",
    "まず、現在の実装では、音符の長さを変えたり、音符間のオフセットを変えたりすることはできません。\n",
    "これを実現するためには、異なる持続時間ごとにクラスを追加し、音符間の休符期間を表す休符クラスを追加することができます。\n",
    "\n",
    "より多くのクラスを追加して満足のいく結果を得るためには、LSTMネットワークの深さを増やす必要があり、そのためにはかなり強力なコンピュータが必要になります。\n",
    "私が自宅で使っているノートパソコンでは、現在のようにネットワークを学習するのに約20時間かかりました。\n",
    "\n",
    "第二に、ピースに始まりと終わりを追加します。現在のネットワークのように、ピースの間には区別がありません。\n",
    "これにより、ネットワークは、現在のように生成されたピースを突然終了させるのではなく、最初から最後までピースを生成することができるようになります。\n",
    "\n",
    "第三に、不明なノートを処理する方法を追加します。\n",
    "現在、ネットワークは知らない音に遭遇するとフェイル状態になります。\n",
    "この問題を解決する方法としては、未知のノートに最も近いノートやコードを見つけることが考えられます。\n",
    "\n",
    "最後に、データセットに楽器を追加します。\n",
    "現在のところ、ネットワークがサポートしているのは単一の楽器のみです。\n",
    "オーケストラ全体をサポートするためにネットワークを拡張できたら面白いでしょう。\n",
    "\n",
    "## 結論\n",
    "このチュートリアルでは、音楽を生成するLSTMニューラルネットワークを作成する方法を示しました。\n",
    "結果は完璧ではないかもしれませんが、それにもかかわらず、かなり印象的なものであり、ニューラルネットワークが音楽を作成し、より複雑な音楽を作成するのに役立つ可能性があることを示しています。\n",
    "\n",
    "[チュートリアルのGithubリポジトリをチェックしてみてください。](https://github.com/Skuldur/Classical-Piano-Composer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
